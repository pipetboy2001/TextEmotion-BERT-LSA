{"cells":[{"cell_type":"markdown","metadata":{"id":"vd82CFhYntkf"},"source":["# Análisis de sentimientos con BERT\n","\n","[Hugging Face](https://huggingface.co/)\n","\n","![BERT análisis sentimientos](https://drive.google.com/uc?export=view&id=1UwciEQKNZ4SoXn_c0l31hsyZ-8jLdtVf)"]},{"cell_type":"markdown","metadata":{"id":"71icBcHoZraa"},"source":["## Instalando BERT"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13631,"status":"ok","timestamp":1687931412358,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"_vbubgPZnoBz","outputId":"41ed295d-bac5-49c4-e2dc-dded4d319fc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"rsASfVPwjztL"},"source":["Se importa BERT model , Bert token y para entrenamiento\n","torch <- red neuronal\n","numpy <- manipular datos\n","sklearn<- dividir set de entrenamiento\n","nn <- red neuronal\n","optim <- optimizacion\n","pandas <- el dataset es un csv\n","textwrap <- visualizar en multiples lineas\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":11230,"status":"ok","timestamp":1687931423575,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"Mc8cWZgOo-fw"},"outputs":[],"source":["from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from textwrap import wrap"]},{"cell_type":"markdown","metadata":{"id":"AmMsdncokZP6"},"source":["- se define una semilla aleatoria para los pesos y asi tener los mismos valores iniciales\n","- la maxima longitud del texto\n","- se introducen solo 16 paquetes para entrenamiento\n","- ruta del set de datos\n","- [negativo] y [positivo]\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1687931423576,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"2YBx1vdgTGV4","outputId":"11666ed6-61b2-4660-de63-2421d1ff66f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# Inicialización\n","RANDOM_SEED = 42\n","MAX_LEN = 200\n","BATCH_SIZE = 16\n","DATASET_PATH = '/content/drive/MyDrive/Colab/Colab Archive/BERT_sentiment_IMDB_Dataset.csv'\n","NCLASSES = 2\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"_h_b2jjilvZX"},"source":["se carga el Dataset original"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34574,"status":"ok","timestamp":1687931458116,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"NPax5SRMUHbL","outputId":"82cdd503-10ff-40c0-ee08-dd7100483237"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Cargar dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df = pd.read_csv(DATASET_PATH)\n","# se toma unicamente los primero 100000 datos\n","df = df[0:10000]"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1687931458117,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"SypONkqHUZGA","outputId":"461b43b1-1b7e-4037-df54-5a2e84523976"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n","(10000, 2)\n","Interesting and short television movie describes some of the\n","machinations surrounding Jay Leno's replacing Carson as host of the\n","Tonight Show. Film is currently very topical given the public drama\n","surrounding Conan O'Brien and Jay Leno.<br /><br />The film does a\n","good job of sparking viewers' interest in the events and showing some\n","of the concerns of the stakeholders, particularly of the NBC\n","executives. The portrayal of Ovitz was particularly compelling and\n","interesting, I thought.<br /><br />Still, many of the characters were\n","only very briefly limned or touched upon, and some of the acting\n","seemed perfunctory. Nevertheless, an interesting story.\n"]}],"source":["print(df.head()) #encabezado\n","print(df.shape)  # tamaño\n","print(\"\\n\".join(wrap(df['review'][200]))) #ejemplo"]},{"cell_type":"markdown","metadata":{"id":"y7QbM69YmuE0"},"source":["- Se hace un cambio para que \"positive\"=1\n","\"negative\" = 0\n","- luego se quita la columna de sentimiento\n","---\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1687931458118,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"UMCr2ZBvUvoX","outputId":"0021c0f4-5ecf-47b0-debb-88138717f046"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review  label\n","0  One of the other reviewers has mentioned that ...      1\n","1  A wonderful little production. <br /><br />The...      1\n","2  I thought this was a wonderful way to spend ti...      1\n","3  Basically there's a family where a little boy ...      0\n","4  Petter Mattei's \"Love in the Time of Money\" is...      1"],"text/html":["\n","  <div id=\"df-3f5694d9-4e99-44fc-ac80-dd40e592d016\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f5694d9-4e99-44fc-ac80-dd40e592d016')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3f5694d9-4e99-44fc-ac80-dd40e592d016 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3f5694d9-4e99-44fc-ac80-dd40e592d016');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}],"source":["# Reajustar dataset\n","df['label'] = (df['sentiment']=='positive').astype(int)\n","df.drop('sentiment', axis=1, inplace=True)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"f2HDS0PknG24"},"source":["`[CLS] [pregunta] [SEP] [Respuesta]` para nuestro caso seria\n","`[CLS] [frase] [SEP] [null]`"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2337,"status":"ok","timestamp":1687931460426,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"yEsKKezJVZEW","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["b30e36a78d4e4e5bab2a3b2caf8c47ee","ba74c58065424240b77d4c566c32b6b2","c8a8b0d67ea542ee8e6d5b0e88a01507","eee12b6387a54aa88383a84154cfe3b4","2a65f93180474fda8ba500ebab5ee765","8cf70b083f154644ad0f1815f40c00a1","66b9266205ba4df49df3fe9805139c0b","2e6dedcabfc04b92baecb352996bac0f","0e5e93d47a524e5fbeb6157311e3d1d1","5cc05f4794864e23a59ae9fb5d8f1da2","e948425766d6476aa34000d519545bd2","00a78a6626df4811bfe87cfa128a8464","8dc878f349d645d39d6024be4ef2f558","5fd5fd94819a4c63b02e70f72d519ca0","7737a081769f49dfa949c680f4a589a4","73bf63aaa27f406bb8660ee38f7a5bc6","6b6f4bebe5444b32b49ee0487ad3fbdc","ac6758e3377845079314a6a1c16cf23f","b927f3858a3a48fdab99a4c722b9b452","991cc3bf8c514c538e9403e007c81898","beb22984f7894e309ad00bc15ca6af89","a549c4905b51439187d4ad2c0a0067d7","6995cfb95dad4ab9b7364542662022cf","3fae4ce87a9c424d9b18cd768fed08f2","9319c3801049487a87d7f218646dfeb3","43b6836e96584cb996818d51aac7b9d2","6b7e649383334bf3bc8e90cc141cb5d6","11c1135e7c3848d88bbd4cee5a442307","939e2f122e9f4c38bc6d853e01f780bf","448c6314def34fbb8058b65d81300325","4575474db2d742f0b8a4f24fd9acff6a","492cc0582a5848b28967143853c48b89","bef7ec433a9a4213b5be3b64f1b31907"]},"outputId":"de3e1870-de68-4d2b-d02c-95d06b64a009"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30e36a78d4e4e5bab2a3b2caf8c47ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a78a6626df4811bfe87cfa128a8464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6995cfb95dad4ab9b7364542662022cf"}},"metadata":{}}],"source":["# TOKENIZACIÓN\n","PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased' #entiende mayus y minus\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"]},{"cell_type":"markdown","metadata":{"id":"oVvK1YxjoNN_"},"source":["### ejemplo de tokenizacion\n","¿de donde viene esta cantidad numerica? del modelo prenetrenado 'bert-base-cased'"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1687931460427,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"LkNsk0GGWWW6","outputId":"ec26ab03-a15c-4d45-9b63-8cb56f27b5a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Frase:  I really loved that movie!\n","Tokens:  ['I', 'really', 'loved', 'that', 'movie', '!']\n","Tokens numéricos:  [146, 30181, 82321, 10189, 18379, 106]\n"]}],"source":["# Ejemplo tokenización\n","sample_txt = 'I really loved that movie!'\n","tokens = tokenizer.tokenize(sample_txt)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print('Frase: ', sample_txt)\n","print('Tokens: ', tokens)\n","print('Tokens numéricos: ', token_ids)"]},{"cell_type":"markdown","metadata":{"id":"iUfNfPmvpLRC"},"source":["Funcion para colocar el [CLS] y [SEP]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687931460427,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"XDqgDsUqW0fO"},"outputs":[],"source":["# Codificación para introducir a BERT\n","encoding = tokenizer.encode_plus(\n","    sample_txt,\n","    max_length=10,\n","    truncation=True,\n","    add_special_tokens=True,\n","    padding='max_length',\n","    return_attention_mask=True,\n","    return_tensors='pt'\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"GgSC-0dupx3H"},"source":["### Dicionario con cada elemnto de la frase codificado con un numero y la mascara que toma lo que me interesa con 1 y lo que no con 0"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687931460428,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"QfE50cS2X8uu","outputId":"ffd33ddd-df28-4134-84ee-b7f4f3ac0cb9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{},"execution_count":16}],"source":["encoding.keys()"]},{"cell_type":"markdown","metadata":{"id":"FpT7c3JmqHyQ"},"source":["se imprime el input ID y la mascara"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1687931460428,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"er5DlrQ9X-mG","outputId":"c9d90cfc-d126-487d-be05-14a451b61b22"},"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', 'I', 'really', 'loved', 'that', 'movie', '!', '[SEP]', '[PAD]', '[PAD]']\n","tensor([  101,   146, 30181, 82321, 10189, 18379,   106,   102,     0,     0])\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n"]}],"source":["print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n","print(encoding['input_ids'][0])\n","print(encoding['attention_mask'][0])"]},{"cell_type":"markdown","metadata":{"id":"xTQuWikSqyC-"},"source":["\\Se crea un DATASET para despues hacer el entrenemiento"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1687931460429,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"pqv-DZtzYQhm"},"outputs":[],"source":["# CREACIÓN DATASET\n","\n","class IMDBDataset(Dataset):\n","\n","  def __init__(self,reviews,labels,tokenizer,max_len):\n","    self.reviews = reviews\n","    self.labels = labels\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","      return len(self.reviews)\n","\n","  def __getitem__(self, item):\n","    review = str(self.reviews[item])\n","    label = self.labels[item]\n","    encoding = tokenizer.encode_plus(\n","        review,\n","        max_length = self.max_len,\n","        truncation = True,\n","        add_special_tokens = True,\n","        return_token_type_ids = False,\n","        pad_to_max_length = True,\n","        return_attention_mask = True,\n","        return_tensors = 'pt'\n","        )\n","\n","\n","    return {\n","          'review': review,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'label': torch.tensor(label, dtype=torch.long)\n","      }\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1687931460429,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"NaYe0FsoaNAv"},"outputs":[],"source":["# Data loader:\n","\n","def data_loader(df, tokenizer, max_len, batch_size):\n","  dataset = IMDBDataset(\n","      reviews = df.review.to_numpy(),\n","      labels = df.label.to_numpy(),\n","      tokenizer = tokenizer,\n","      max_len = MAX_LEN\n","  )\n","\n","  return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 2)"]},{"cell_type":"markdown","metadata":{"id":"X9__u3sMscD9"},"source":["se divide el set de datos\n","- entrenamiento\n","- validacion"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1687931460430,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"m9rNI4Tsa5MZ"},"outputs":[],"source":["df_train, df_test = train_test_split(df, test_size = 0.2, random_state=RANDOM_SEED)\n","\n","train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"TnljCcmktBH4"},"source":["Crear la clase con las carateristicas de una red neuronal\n","\n","- aqui tenemos nuestro clasificador que tiene 2 cosas\n","el modelo bert + una red neuronal\n","\n","-entonces se carga el modelo prentrenado (transformer)\n","- capa adicinal para apagar el 30% de las neuronas y hacer generar una precision muy similar\n","- una capa de neurona (liner)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1687931460430,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"VOnca-sVbhPX"},"outputs":[],"source":["# EL MODELO!\n","\n","class BERTSentimentClassifier(nn.Module):\n","\n","  def __init__(self, n_classes):\n","    super(BERTSentimentClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,return_dict=False)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n","\n","  def forward(self, input_ids, attention_mask):\n","    _, cls_output = self.bert(\n","        input_ids = input_ids,\n","        attention_mask = attention_mask\n","    )\n","    drop_output = self.drop(cls_output)\n","    output = self.linear(drop_output)\n","    return output\n","\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["1092d9cf61c7416c8a7d8976a2f39e59","90e8804acdee42d0865608a03ce293ec","ceb719e9acaf41dbb20fdaae6a3fcc45","94ddda84dffd45cb97e35f2805377bc2","4a946bdd0a644f388cab0703a721f1de","3d807a9abc32472c8de8224e5ef69980","294d1a9112e24606a60a2e8a99ae05a5","fda852d76a7f4a7e9c48f2f3f954c9d9","7fec15a77bb94f19a9d5a19944b1092b","418183bf61154a12935806a925e0ff33","7c29742623be42f6a0c48a72d477cc36"]},"executionInfo":{"elapsed":9394,"status":"ok","timestamp":1687931469812,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"CanuNncKdXaF","outputId":"ad820a9e-9b52-490a-e3bf-f8eb1549b658"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1092d9cf61c7416c8a7d8976a2f39e59"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model = BERTSentimentClassifier(NCLASSES)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1687931469814,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"S6-VlpvidgLp","outputId":"c4e60962-aa68-4be6-eba0-2b91047f3e0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["BERTSentimentClassifier(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (linear): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}],"source":["print(model)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1687931469815,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"Mansb6pfdn_H","outputId":"a8873103-8326-4703-ef13-d2392b930978"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# ENTRENAMIENTO\n","EPOCHS = 5\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1687931469816,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"},"user_tz":240},"id":"ihoNOG0Zeuhz"},"outputs":[],"source":["# Iteración entrenamiento\n","def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","  model = model.train()\n","  losses = []\n","  correct_predictions = 0\n","  for batch in data_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['label'].to(device)\n","    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, labels)\n","    correct_predictions += torch.sum(preds == labels)\n","    losses.append(loss.item())\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","  return correct_predictions.double()/n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","  losses = []\n","  correct_predictions = 0\n","  with torch.no_grad():\n","    for batch in data_loader:\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      labels = batch['label'].to(device)\n","      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, labels)\n","      correct_predictions += torch.sum(preds == labels)\n","      losses.append(loss.item())\n","  return correct_predictions.double()/n_examples, np.mean(losses)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0aJRM-ZiXtp","executionInfo":{"status":"ok","timestamp":1687933059126,"user_tz":240,"elapsed":1589323,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"}},"outputId":"7865de1e-15c0-4671-b715-c1bced81b9a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 de 5\n","------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Entrenamiento: Loss: 0.6113311282992363, accuracy: 0.6555\n","Validación: Loss: 0.5100473816394806, accuracy: 0.807\n","\n","Epoch 2 de 5\n","------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Entrenamiento: Loss: 0.42935537112504246, accuracy: 0.81575\n","Validación: Loss: 0.41340519380569457, accuracy: 0.811\n","\n","Epoch 3 de 5\n","------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Entrenamiento: Loss: 0.34119863421097396, accuracy: 0.8685\n","Validación: Loss: 0.48735852289199827, accuracy: 0.8190000000000001\n","\n","Epoch 4 de 5\n","------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Entrenamiento: Loss: 0.3005054582543671, accuracy: 0.893875\n","Validación: Loss: 0.4860009580552578, accuracy: 0.835\n","\n","Epoch 5 de 5\n","------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Entrenamiento: Loss: 0.2634133669361472, accuracy: 0.9107500000000001\n","Validación: Loss: 0.541758440092206, accuracy: 0.8325\n","\n"]}],"source":["# Entrenamiento!!!\n","\n","for epoch in range(EPOCHS):\n","  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n","  print('------------------')\n","  train_acc, train_loss = train_model(\n","      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n","  )\n","  test_acc, test_loss = eval_model(\n","      model, test_data_loader, loss_fn, device, len(df_test)\n","  )\n","  print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n","  print('Validación: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n","  print('')"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"QTG7YKnVjDA-","executionInfo":{"status":"ok","timestamp":1687933059127,"user_tz":240,"elapsed":38,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"}}},"outputs":[],"source":["def classifySentiment(review_text):\n","  encoding_review = tokenizer.encode_plus(\n","      review_text,\n","      max_length = MAX_LEN,\n","      truncation = True,\n","      add_special_tokens = True,\n","      return_token_type_ids = False,\n","      pad_to_max_length = True,\n","      return_attention_mask = True,\n","      return_tensors = 'pt'\n","      )\n","\n","  input_ids = encoding_review['input_ids'].to(device)\n","  attention_mask = encoding_review['attention_mask'].to(device)\n","  output = model(input_ids, attention_mask)\n","  _, prediction = torch.max(output, dim=1)\n","  print(\"\\n\".join(wrap(review_text)))\n","  if prediction:\n","    print('Sentimiento predicho: Positive')\n","  else:\n","    print('Sentimiento predicho: Negative')\n","\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oe0im1_alf4K","executionInfo":{"status":"ok","timestamp":1687933059127,"user_tz":240,"elapsed":18,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"}},"outputId":"7fbb3582-1d28-4c65-c971-c9b76ffac627"},"outputs":[{"output_type":"stream","name":"stdout","text":["Avengers: Infinity War at least had the good taste to abstain from\n","Jeremy Renner. No such luck in Endgame.\n","Sentimiento predicho: Negative\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["review_text = \"Avengers: Infinity War at least had the good taste to abstain from Jeremy Renner. No such luck in Endgame.\"\n","\n","classifySentiment(review_text)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"rNlHECVBuDXJ","executionInfo":{"status":"ok","timestamp":1687933059128,"user_tz":240,"elapsed":13,"user":{"displayName":"Felipe Fernandez","userId":"07104336536485052592"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b30e36a78d4e4e5bab2a3b2caf8c47ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba74c58065424240b77d4c566c32b6b2","IPY_MODEL_c8a8b0d67ea542ee8e6d5b0e88a01507","IPY_MODEL_eee12b6387a54aa88383a84154cfe3b4"],"layout":"IPY_MODEL_2a65f93180474fda8ba500ebab5ee765"}},"ba74c58065424240b77d4c566c32b6b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cf70b083f154644ad0f1815f40c00a1","placeholder":"​","style":"IPY_MODEL_66b9266205ba4df49df3fe9805139c0b","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"c8a8b0d67ea542ee8e6d5b0e88a01507":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e6dedcabfc04b92baecb352996bac0f","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e5e93d47a524e5fbeb6157311e3d1d1","value":995526}},"eee12b6387a54aa88383a84154cfe3b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cc05f4794864e23a59ae9fb5d8f1da2","placeholder":"​","style":"IPY_MODEL_e948425766d6476aa34000d519545bd2","value":" 996k/996k [00:00&lt;00:00, 19.8MB/s]"}},"2a65f93180474fda8ba500ebab5ee765":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cf70b083f154644ad0f1815f40c00a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66b9266205ba4df49df3fe9805139c0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e6dedcabfc04b92baecb352996bac0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e5e93d47a524e5fbeb6157311e3d1d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cc05f4794864e23a59ae9fb5d8f1da2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e948425766d6476aa34000d519545bd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00a78a6626df4811bfe87cfa128a8464":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8dc878f349d645d39d6024be4ef2f558","IPY_MODEL_5fd5fd94819a4c63b02e70f72d519ca0","IPY_MODEL_7737a081769f49dfa949c680f4a589a4"],"layout":"IPY_MODEL_73bf63aaa27f406bb8660ee38f7a5bc6"}},"8dc878f349d645d39d6024be4ef2f558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b6f4bebe5444b32b49ee0487ad3fbdc","placeholder":"​","style":"IPY_MODEL_ac6758e3377845079314a6a1c16cf23f","value":"Downloading (…)okenizer_config.json: 100%"}},"5fd5fd94819a4c63b02e70f72d519ca0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b927f3858a3a48fdab99a4c722b9b452","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_991cc3bf8c514c538e9403e007c81898","value":29}},"7737a081769f49dfa949c680f4a589a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beb22984f7894e309ad00bc15ca6af89","placeholder":"​","style":"IPY_MODEL_a549c4905b51439187d4ad2c0a0067d7","value":" 29.0/29.0 [00:00&lt;00:00, 1.50kB/s]"}},"73bf63aaa27f406bb8660ee38f7a5bc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6f4bebe5444b32b49ee0487ad3fbdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac6758e3377845079314a6a1c16cf23f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b927f3858a3a48fdab99a4c722b9b452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"991cc3bf8c514c538e9403e007c81898":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"beb22984f7894e309ad00bc15ca6af89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a549c4905b51439187d4ad2c0a0067d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6995cfb95dad4ab9b7364542662022cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fae4ce87a9c424d9b18cd768fed08f2","IPY_MODEL_9319c3801049487a87d7f218646dfeb3","IPY_MODEL_43b6836e96584cb996818d51aac7b9d2"],"layout":"IPY_MODEL_6b7e649383334bf3bc8e90cc141cb5d6"}},"3fae4ce87a9c424d9b18cd768fed08f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11c1135e7c3848d88bbd4cee5a442307","placeholder":"​","style":"IPY_MODEL_939e2f122e9f4c38bc6d853e01f780bf","value":"Downloading (…)lve/main/config.json: 100%"}},"9319c3801049487a87d7f218646dfeb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_448c6314def34fbb8058b65d81300325","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4575474db2d742f0b8a4f24fd9acff6a","value":625}},"43b6836e96584cb996818d51aac7b9d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_492cc0582a5848b28967143853c48b89","placeholder":"​","style":"IPY_MODEL_bef7ec433a9a4213b5be3b64f1b31907","value":" 625/625 [00:00&lt;00:00, 27.3kB/s]"}},"6b7e649383334bf3bc8e90cc141cb5d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11c1135e7c3848d88bbd4cee5a442307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939e2f122e9f4c38bc6d853e01f780bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"448c6314def34fbb8058b65d81300325":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4575474db2d742f0b8a4f24fd9acff6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"492cc0582a5848b28967143853c48b89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bef7ec433a9a4213b5be3b64f1b31907":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1092d9cf61c7416c8a7d8976a2f39e59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90e8804acdee42d0865608a03ce293ec","IPY_MODEL_ceb719e9acaf41dbb20fdaae6a3fcc45","IPY_MODEL_94ddda84dffd45cb97e35f2805377bc2"],"layout":"IPY_MODEL_4a946bdd0a644f388cab0703a721f1de"}},"90e8804acdee42d0865608a03ce293ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d807a9abc32472c8de8224e5ef69980","placeholder":"​","style":"IPY_MODEL_294d1a9112e24606a60a2e8a99ae05a5","value":"Downloading model.safetensors: 100%"}},"ceb719e9acaf41dbb20fdaae6a3fcc45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fda852d76a7f4a7e9c48f2f3f954c9d9","max":714290682,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fec15a77bb94f19a9d5a19944b1092b","value":714290682}},"94ddda84dffd45cb97e35f2805377bc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_418183bf61154a12935806a925e0ff33","placeholder":"​","style":"IPY_MODEL_7c29742623be42f6a0c48a72d477cc36","value":" 714M/714M [00:02&lt;00:00, 239MB/s]"}},"4a946bdd0a644f388cab0703a721f1de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d807a9abc32472c8de8224e5ef69980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"294d1a9112e24606a60a2e8a99ae05a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fda852d76a7f4a7e9c48f2f3f954c9d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fec15a77bb94f19a9d5a19944b1092b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"418183bf61154a12935806a925e0ff33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c29742623be42f6a0c48a72d477cc36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}